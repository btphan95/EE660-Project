{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f7d07ed4b55e5e538a2bf78dfbb38c7822b2da98"
   },
   "source": [
    "# PUBG Finish Placement Prediction\n",
    "PlayerUnknown's BattleGrounds (PUBG) has enjoyed massive popularity. With over 50 million copies sold, it's the fifth best selling game of all time, and has millions of active monthly players. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "#EE 660 Project\n",
    "# Created by Binh Phan\n",
    "\n",
    "# For autoreloading modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# # For notebook plotting\n",
    "# %matplotlib inline\n",
    "\n",
    "#Standard libraries\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "# Visualization\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "from scipy.cluster import hierarchy as hc\n",
    "\n",
    "#Machine Learning\n",
    "\n",
    "import sklearn\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "# import tensorflow as tf\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Dropout, BatchNormalization\n",
    "# from keras.callbacks import EarlyStopping\n",
    "# from keras import optimizers\n",
    "# from keras import regularizers\n",
    "import xgboost as xgb\n",
    "# import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": false,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/train_V2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2e798a7a2882728c239ee5e945203e9859527a30"
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a090dea34b13dd3efbe3e572cd1d21efa8526439"
   },
   "outputs": [],
   "source": [
    "# Remove Id, which is not a useful feature\n",
    "train.drop(columns=['Id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "649a795067aa49d2069a5658fbf6234fc114c71e"
   },
   "outputs": [],
   "source": [
    "train.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "218ec2772f5fb1988418306b3803fcb7cddabe16"
   },
   "outputs": [],
   "source": [
    "train[train['winPlacePerc'].isnull()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "723ace3dea35f7d6cac56845866750c1f39ae8d5"
   },
   "outputs": [],
   "source": [
    "train = train.dropna()\n",
    "train.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f9cb4cef71ca7a45885fc1591da17b938f4b4fbc"
   },
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7cacefdc196f028fbe0b679c2a5d749c07d9e952"
   },
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "641f7dc65e2e94073041a8e6887837e6cfaa9e47"
   },
   "outputs": [],
   "source": [
    "print('There are {} different Match types in the dataset.'.format(train['matchType'].nunique()))\n",
    "\n",
    "# One hot encode matchType\n",
    "train = pd.get_dummies(train, columns=['matchType'])\n",
    "\n",
    "# Take a look at the encoding\n",
    "matchType_encoding = train.filter(regex='matchType')\n",
    "matchType_encoding.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f77d50c0c22d4f34cc5021f674478a0b996b4f48"
   },
   "outputs": [],
   "source": [
    "# Turn groupId and match Id into categorical types\n",
    "train['groupId'] = train['groupId'].astype('category')\n",
    "train['matchId'] = train['matchId'].astype('category')\n",
    "\n",
    "# Get category coding for groupId and matchID\n",
    "train['groupId_cat'] = train['groupId'].cat.codes\n",
    "train['matchId_cat'] = train['matchId'].cat.codes\n",
    "\n",
    "# Get rid of old columns\n",
    "train.drop(columns=['groupId', 'matchId'], inplace=True)\n",
    "\n",
    "# Lets take a look at our newly created features\n",
    "train[['groupId_cat', 'matchId_cat']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a6967e8dc495ff467ed0551ef53f1aadf195719c"
   },
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5bc33d60c3036d1e36d9bd33f5024dff1d23352d"
   },
   "outputs": [],
   "source": [
    "# Take sample for debugging and exploration\n",
    "sample = 50000\n",
    "df_sample = train.sample(sample)\n",
    "\n",
    "# Split sample into training data and target variable\n",
    "df = df_sample.drop(columns = ['winPlacePerc']) #all columns except target\n",
    "y = df_sample['winPlacePerc'] # Only target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f99a3b6d0cb69d06bb1e34c9704054546e828e87"
   },
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "corr = df.corr()\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Create heatmap\n",
    "heatmap = sns.heatmap(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fad029821702706767e7e1a24c8f193c2dcb676d"
   },
   "outputs": [],
   "source": [
    "# Create a Dendrogram to view highly correlated features\n",
    "corr = np.round(scipy.stats.spearmanr(df).correlation, 4)\n",
    "corr_condensed = hc.distance.squareform(1-corr)\n",
    "z = hc.linkage(corr_condensed, method='average')\n",
    "fig = plt.figure(figsize=(14,10))\n",
    "dendrogram = hc.dendrogram(z, labels=df.columns, orientation='left', leaf_font_size=16)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "56ad1edf07e4207b6e421f344329e4e30f8804a6"
   },
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "df97a5529ba5eed6cb90be3587553be859d82e98"
   },
   "outputs": [],
   "source": [
    "#Import data\n",
    "train = pd.read_csv('../input/train_V2.csv')\n",
    "\n",
    "train = train.sample(50000)\n",
    "X_test = pd.read_csv('../input/test_V2.csv')\n",
    "# Remove Id, which is not a useful feature\n",
    "train.drop(columns=['Id'], inplace=True)\n",
    "train = train.dropna()\n",
    "# One hot encode matchType\n",
    "train = pd.get_dummies(train, columns=['matchType'])\n",
    "X_test = pd.get_dummies(X_test, columns=['matchType'])\n",
    "# Turn groupId and match Id into categorical types\n",
    "train['groupId'] = train['groupId'].astype('category')\n",
    "train['matchId'] = train['matchId'].astype('category')\n",
    "X_test['groupId'] = X_test['groupId'].astype('category')\n",
    "X_test['matchId'] = X_test['matchId'].astype('category')\n",
    "\n",
    "# Get category coding for groupId and matchID\n",
    "train['groupId_cat'] = train['groupId'].cat.codes\n",
    "train['matchId_cat'] = train['matchId'].cat.codes\n",
    "X_test['groupId_cat'] = X_test['groupId'].cat.codes\n",
    "X_test['matchId_cat'] = X_test['matchId'].cat.codes\n",
    "\n",
    "# Get rid of old columns\n",
    "train.drop(columns=['groupId', 'matchId'], inplace=True)\n",
    "X_test.drop(columns=['groupId', 'matchId'], inplace=True)\n",
    "\n",
    "# Split train into features and target variable\n",
    "X_train = train.drop(columns = ['winPlacePerc']) #all columns except target\n",
    "Y = train['winPlacePerc'] # Only target variable\n",
    "Y = Y.astype('float32')\n",
    "x_train, x_val, y_train, y_val = sklearn.model_selection.train_test_split(X_train, Y, random_state=0, test_size=0.3)\n",
    "\n",
    "#Standard scaling train features to have 0 mean and 1 variance\n",
    "columns_to_scale = ['assists', 'boosts', 'damageDealt', 'DBNOs', 'headshotKills', 'heals',\n",
    "       'killPlace', 'killPoints', 'kills', 'killStreaks', 'longestKill',\n",
    "       'matchDuration', 'maxPlace', 'numGroups', 'rankPoints', 'revives',\n",
    "       'rideDistance', 'roadKills', 'swimDistance', 'teamKills',\n",
    "       'vehicleDestroys', 'walkDistance', 'weaponsAcquired', 'winPoints']\n",
    "categorical  = ['matchType_crashfpp', 'matchType_crashtpp', 'matchType_duo',\n",
    "       'matchType_duo-fpp', 'matchType_flarefpp', 'matchType_flaretpp',\n",
    "       'matchType_normal-duo', 'matchType_normal-duo-fpp',\n",
    "       'matchType_normal-solo', 'matchType_normal-solo-fpp',\n",
    "       'matchType_normal-squad', 'matchType_normal-squad-fpp',\n",
    "       'matchType_solo', 'matchType_solo-fpp', 'matchType_squad',\n",
    "       'matchType_squad-fpp', 'groupId_cat', 'matchId_cat']\n",
    "train_scale = x_train[columns_to_scale]\n",
    "train_categorical = x_train[categorical]\n",
    "val_scale = x_val[columns_to_scale]\n",
    "val_categorical = x_val[categorical]\n",
    "test_scale = X_test[columns_to_scale]\n",
    "test_categorical = X_test[categorical]\n",
    "# print(df_scale.shape)\n",
    "# print(df_categorical.shape)\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "scaler.fit(train_scale.values)\n",
    "\n",
    "#Standard scaling train features to have 0 mean and 1 variance\n",
    "train_scale = pd.DataFrame(scaler.transform(train_scale.values), index=train_scale.index, columns=train_scale.columns)\n",
    "# print(df_scale.shape)\n",
    "x_train = pd.concat([train_scale, train_categorical],axis=1)\n",
    "\n",
    "#Standard scaling val features with the same parameters in train data\n",
    "val_scale = pd.DataFrame(scaler.transform(val_scale.values), index=val_scale.index, columns=val_scale.columns)\n",
    "# print(df_scale.shape)\n",
    "x_val = pd.concat([val_scale, val_categorical],axis=1)\n",
    "\n",
    "#Standard scaling test features with the same parameters in train data\n",
    "test_scale = pd.DataFrame(scaler.transform(test_scale.values), index=test_scale.index, columns=test_scale.columns)\n",
    "# print(df_scale.shape)\n",
    "\n",
    "test_id = X_test.loc[:, ['Id']]\n",
    "X_test = pd.concat([test_scale, test_categorical],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b71d9a3ca906ed13804f4414031e2373ef301d80"
   },
   "source": [
    "## Reducing Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "253bee939ad3e1f10f65c7da54366400d8877a62"
   },
   "outputs": [],
   "source": [
    "# Thanks and credited to https://www.kaggle.com/gemartin who created this wonderful mem reducer\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() \n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() \n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    "\n",
    "x_train = reduce_mem_usage(x_train)\n",
    "x_val = reduce_mem_usage(x_val)\n",
    "X_test = reduce_mem_usage(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3977102fb37c5c34fe8a6194022c233ff7fa5289"
   },
   "source": [
    "## Baseline: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a983bc986a6cd319540e85fae984195e4208315b"
   },
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LinearRegression\n",
    "reg = sklearn.linear_model.LinearRegression().fit(x_train, y_train)\n",
    "pred_train = reg.predict(x_train)\n",
    "pred_val = reg.predict(x_val)\n",
    "mae_train = sklearn.metrics.mean_absolute_error(pred_train, y_train)\n",
    "mae_val = sklearn.metrics.mean_absolute_error(pred_val, y_val)\n",
    "print('MAE train: ', mae_train)\n",
    "print('MAE val: ', mae_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "20f74daa748d3f02fd12afbd958b19ff661dcc29"
   },
   "source": [
    "## Boosted Trees Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d60e648101061338295cbb8ea4401c03303bc874"
   },
   "outputs": [],
   "source": [
    "trees = GradientBoostingRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5e4333b79cb45fa52d9dbaaf257a7c4ab9e7c476"
   },
   "outputs": [],
   "source": [
    "trees.fit(x_train, y_train)\n",
    "pred_train = trees.predict(x_train)\n",
    "pred_val = trees.predict(x_val)\n",
    "mae_train = sklearn.metrics.mean_absolute_error(pred_train, y_train)\n",
    "mae_val = sklearn.metrics.mean_absolute_error(pred_val, y_val)\n",
    "print('MAE train: ', mae_train)\n",
    "print('MAE val: ', mae_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "99e0f7ba9e4aa1e61b2c77853dbcb4a587d21470"
   },
   "outputs": [],
   "source": [
    "rtrees = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
    "           max_features='auto', max_leaf_nodes=None,\n",
    "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "           min_samples_leaf=1, min_samples_split=2,\n",
    "           min_weight_fraction_leaf=0.0, n_estimators=3, n_jobs=None,\n",
    "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
    "rtrees.fit(x_train, y_train)\n",
    "y_pred = rtrees.predict(x_val)\n",
    "mae = sklearn.metrics.mean_absolute_error(y_pred, y_val)\n",
    "mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1748e2b5ae60e155646b16ea0f8f00484aacfeb6"
   },
   "source": [
    "## Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "638b603107f6e9c3a355c5c9a0faee087b81f44d"
   },
   "outputs": [],
   "source": [
    "# BATCH_SIZE = 256\n",
    "# EPOCHS = 50\n",
    "# LEARNING_RATE = 0.001\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Dense(128, activation='relu', input_dim=x_train.shape[1], activity_regularizer=regularizers.l1(0.01)))\n",
    "# # model.add(BatchNormalization())\n",
    "# # model.add(Dense(128, activation='relu'))\n",
    "# # model.add(BatchNormalization())\n",
    "# # model.add(Dense(64, activation='relu'))\n",
    "# # model.add(BatchNormalization())\n",
    "# # model.add(Dense(32, activation='relu'))\n",
    "# # model.add(BatchNormalization())\n",
    "# model.add(Dense(8, activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dense(1))\n",
    "\n",
    "# adam = optimizers.adam(lr=LEARNING_RATE)\n",
    "# model.compile(loss='mse', optimizer=adam, metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b849a0e6cefcec6c510faf5754e4cd40511e8221"
   },
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "53254284bd5b9f79833d3015e98a64c6a68239d5"
   },
   "outputs": [],
   "source": [
    "# history = model.fit(x=x_train, y=y_train, batch_size=BATCH_SIZE, epochs=20, \n",
    "#                     verbose=1, validation_data=(x_val,y_val), \n",
    "#                     shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bd37ab06148900bcbf7d78ad54ee1d25ce67a6eb"
   },
   "outputs": [],
   "source": [
    "# def plot_loss_accuracy(history):\n",
    "#     plt.figure(figsize=(10,10))\n",
    "#     plt.plot(history.history['loss'])\n",
    "#     plt.plot(history.history['val_loss'])\n",
    "#     plt.title('model loss')\n",
    "#     plt.ylabel('loss')\n",
    "#     plt.xlabel('epoch')\n",
    "#     plt.legend(['train', 'test'], loc='upper right')\n",
    "#     plt.show()\n",
    "# plot_loss_accuracy(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e84311aa9e57154943c31262ef1f5b79ead5f228"
   },
   "outputs": [],
   "source": [
    "# X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2d1cd78fc809c0d8785d2431e5437610f8f58158"
   },
   "outputs": [],
   "source": [
    "# prediction = model.predict(X_test, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "04e6f0aec9d02de6ee62329dd4b70ce8cc744795"
   },
   "outputs": [],
   "source": [
    "# prediction = prediction.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "da351aa7ca442394ece598b9d38a5af9b014cc9b"
   },
   "outputs": [],
   "source": [
    "# prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cff1a0c595e60b5333084ba51af533062c6ea72a"
   },
   "outputs": [],
   "source": [
    "# pred_df = pd.DataFrame({'Id' : test_id['Id'], 'winPlacePerc' : prediction})\n",
    "\n",
    "# # Create submission file\n",
    "# pred_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ee7c226ca610ef7c3bef4c166cb53864dc5ddd79"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d9997fbf909f1a47c0415a35b030dd8db4881a8e"
   },
   "source": [
    "## Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9b9075a00106c96279699015ca95fddfe7227fcb"
   },
   "outputs": [],
   "source": [
    "# # Model parameters\n",
    "# BATCH_SIZE = 512\n",
    "# STEPS = 40000\n",
    "# LEARNING_RATE = 0.001\n",
    "# HIDDEN_UNITS = [256, 128, 64, 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "88150df9e2342436e39655864e8612622cdf4394"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c1d97e83b5bb02366d15d82c0a2a7e0c8b3dc7cd"
   },
   "outputs": [],
   "source": [
    "# feature_columns = [tf.feature_column.numeric_column(x) for x in x_train.columns]\n",
    "\n",
    "# train_fn = tf.estimator.inputs.pandas_input_fn(x_train, y_train, shuffle = True)\n",
    "# val_fn = tf.estimator.inputs.pandas_input_fn(x_val, y_val, shuffle = False)\n",
    "# test_fn = tf.estimator.inputs.pandas_input_fn(X_test, None, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8b0d97c0bd3ba366b82187b02b31041109ad65cc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8aee69eacc65c7ef66f316c5f9f99cfed7a88394"
   },
   "outputs": [],
   "source": [
    "# # optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
    "# optimizer = tf.train.ProximalAdagradOptimizer(learning_rate=0.1, l1_regularization_strength=0.001, l2_regularization_strength=0.001)\n",
    "# estimator = tf.estimator.DNNLinearCombinedRegressor(\n",
    "#     dnn_feature_columns=feature_columns,\n",
    "#     dnn_hidden_units=HIDDEN_UNITS,\n",
    "#     dnn_optimizer=optimizer)\n",
    "# train_spec = tf.estimator.TrainSpec(train_fn, max_steps=STEPS)\n",
    "# eval_spec = tf.estimator.EvalSpec(val_fn, steps=500, throttle_secs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "db9b460a1366f163b2fa46f7aea5df5d7f04c872"
   },
   "outputs": [],
   "source": [
    "# tf.estimator.train_and_evaluate(estimator, train_spec=train_spec, eval_spec=eval_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e3d66182921ed649864df286da698ae9a196e15e"
   },
   "outputs": [],
   "source": [
    "# prediction = estimator.predict(test_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "30eb3f49bb12417ed226bdc7558e2a551743b673"
   },
   "outputs": [],
   "source": [
    "# prediction_df = pd.DataFrame(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7564aa8d457629b83736a9250c6a90e1b8fe3df0"
   },
   "outputs": [],
   "source": [
    "# prediction_df.values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "30a9520d0394122096fd37010c667a62142cbaaa"
   },
   "outputs": [],
   "source": [
    "# pred_df = pd.DataFrame({'Id' : test_id['Id'], 'winPlacePerc' : prediction_df.values.flatten()})\n",
    "\n",
    "# # Create submission file\n",
    "# pred_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cc1821e2371247f4249cf4e1ae5f1f8ea65671c4"
   },
   "outputs": [],
   "source": [
    "# def mae(labels, predictions):\n",
    "#     pred_values = predictions['predictions']\n",
    "# #     print type(pred_values)\n",
    "#     return {'mae': tf.metrics.mean_absolute_error(labels, pred_values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b4e7f49763bd2d7274a5f529a021800a7ad48da2"
   },
   "outputs": [],
   "source": [
    "# DNN = tf.estimator.DNNRegressor(\n",
    "#     feature_columns=feature_columns,\n",
    "#     hidden_units=[2048, 1024,256, 128, 64, 32,4,1],\n",
    "#     dropout = 0.1,\n",
    "#     optimizer=tf.train.ProximalAdagradOptimizer(\n",
    "#       learning_rate=0.1,\n",
    "#       l1_regularization_strength=0.001\n",
    "#     ))\n",
    "# DNN = tf.contrib.estimator.add_metrics(DNN, mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "01f9bc2e8e2d5ee9d3a0ccacfd0606fc5b7ac4ce"
   },
   "outputs": [],
   "source": [
    "# DNN.train(train_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fa4c96cbfcec6d659c63f6bae375a24aaa5f3fcd"
   },
   "outputs": [],
   "source": [
    "# DNN.evaluate(val_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9003cde4e6ce4d119351152fa3573116513044e1"
   },
   "outputs": [],
   "source": [
    "# predictions = np.array([item['predictions'][0] for item in preds]).astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f0e3073998904f05b1dedf538cf763e6da386a7d"
   },
   "outputs": [],
   "source": [
    "# yvalnp = np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f8c504bd3dcb4d97f88566c1261440e71d5e5e5a"
   },
   "outputs": [],
   "source": [
    "# type(yvalnp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "366ea647514d9a1bfec092772b3b0b105464bf64"
   },
   "outputs": [],
   "source": [
    "# type(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ac38aee42c29c33a1246fe35511bf4e14245f728"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "35123b294e0496ede1c0d822e91cbe70e116691d"
   },
   "outputs": [],
   "source": [
    "# mae = sklearn.metrics.mean_absolute_error(yvalnp, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "262efe2bc7672fbaf370544f35446e0afc5b91ae"
   },
   "outputs": [],
   "source": [
    "# mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a76414a4c31fb59645dcf58785833d304d81d7ca"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "209b2d13607a397dca2d93ebf387096534f6da1e"
   },
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e268663111b70af95c944b81cc03d25d420f5a25"
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "78bcb51d58aa6e9f7535aa8297112de8019ff544"
   },
   "outputs": [],
   "source": [
    "#Import data\n",
    "\n",
    "train = pd.read_csv('../input/train_V2.csv')\n",
    "\n",
    "train = train.sample(100000)\n",
    "X_test = pd.read_csv('../input/test_V2.csv')\n",
    "# Remove Id, which is not a useful feature\n",
    "train.drop(columns=['Id'], inplace=True)\n",
    "train = train.dropna()\n",
    "# One hot encode matchType\n",
    "train = pd.get_dummies(train, columns=['matchType'])\n",
    "X_test = pd.get_dummies(X_test, columns=['matchType'])\n",
    "# Turn groupId and match Id into categorical types\n",
    "train['groupId'] = train['groupId'].astype('category')\n",
    "train['matchId'] = train['matchId'].astype('category')\n",
    "X_test['groupId'] = X_test['groupId'].astype('category')\n",
    "X_test['matchId'] = X_test['matchId'].astype('category')\n",
    "\n",
    "# Get category coding for groupId and matchID\n",
    "train['groupId_cat'] = train['groupId'].cat.codes\n",
    "train['matchId_cat'] = train['matchId'].cat.codes\n",
    "X_test['groupId_cat'] = X_test['groupId'].cat.codes\n",
    "X_test['matchId_cat'] = X_test['matchId'].cat.codes\n",
    "\n",
    "# Get rid of old columns\n",
    "train.drop(columns=['groupId', 'matchId'], inplace=True)\n",
    "X_test.drop(columns=['groupId', 'matchId'], inplace=True)\n",
    "\n",
    "# Split train into features and target variable\n",
    "X_train = train.drop(columns = ['winPlacePerc']) #all columns except target\n",
    "Y = train['winPlacePerc'] # Only target variable\n",
    "Y = Y.astype('float32')\n",
    "\n",
    "#Standard scaling train features to have 0 mean and 1 variance\n",
    "columns_to_scale = ['assists', 'boosts', 'damageDealt', 'DBNOs', 'headshotKills', 'heals',\n",
    "       'killPlace', 'killPoints', 'kills', 'killStreaks', 'longestKill',\n",
    "       'matchDuration', 'maxPlace', 'numGroups', 'rankPoints', 'revives',\n",
    "       'rideDistance', 'roadKills', 'swimDistance', 'teamKills',\n",
    "       'vehicleDestroys', 'walkDistance', 'weaponsAcquired', 'winPoints']\n",
    "categorical  = ['matchType_crashfpp', 'matchType_crashtpp', 'matchType_duo',\n",
    "       'matchType_duo-fpp', 'matchType_flarefpp', 'matchType_flaretpp',\n",
    "       'matchType_normal-duo', 'matchType_normal-duo-fpp',\n",
    "       'matchType_normal-solo', 'matchType_normal-solo-fpp',\n",
    "       'matchType_normal-squad', 'matchType_normal-squad-fpp',\n",
    "       'matchType_solo', 'matchType_solo-fpp', 'matchType_squad',\n",
    "       'matchType_squad-fpp', 'groupId_cat', 'matchId_cat']\n",
    "train_scale = X_train[columns_to_scale]\n",
    "train_categorical = X_train[categorical]\n",
    "test_scale = X_test[columns_to_scale]\n",
    "test_categorical = X_test[categorical]\n",
    "# print(df_scale.shape)\n",
    "# print(df_categorical.shape)\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "scaler.fit(train_scale.values)\n",
    "\n",
    "#Standard scaling train features to have 0 mean and 1 variance\n",
    "train_scale = pd.DataFrame(scaler.transform(train_scale.values), index=train_scale.index, columns=train_scale.columns)\n",
    "# print(df_scale.shape)\n",
    "X_train = pd.concat([train_scale, train_categorical],axis=1)\n",
    "\n",
    "#Standard scaling test features with the same parameters in train data\n",
    "test_scale = pd.DataFrame(scaler.transform(test_scale.values), index=test_scale.index, columns=test_scale.columns)\n",
    "# print(df_scale.shape)\n",
    "\n",
    "test_id = X_test.loc[:, ['Id']]\n",
    "X_test = pd.concat([test_scale, test_categorical],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "030075b9f375d14708336c37a298f072c3420c6c"
   },
   "source": [
    "### Reducing Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6ffb0238a4aab3cc5050dfda85b2738de80caf68"
   },
   "outputs": [],
   "source": [
    "# Thanks and credited to https://www.kaggle.com/gemartin who created this wonderful mem reducer\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() \n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() \n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    "\n",
    "X_train = reduce_mem_usage(X_train)\n",
    "X_test = reduce_mem_usage(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "47428cc5c17055dc2d3f85e9ae68862f089b237e"
   },
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "2e68bd98582fba700085345336729bfba636a8a6",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Cross-validation\n",
    "params = {\n",
    "    # Parameters that we are going to tune.\n",
    "    'eta': 0.28, #Result of tuning with CV\n",
    "    'max_depth': 5, #Result of tuning with CV\n",
    "    'subsample': 1, #Result of tuning with CV\n",
    "    'lambda': 0.01, #Result of tuning with CV\n",
    "    'colsample_bytree': 0.5, #Result of tuning with CV\n",
    "    # Other parameters\n",
    "    'objective':'reg:linear',\n",
    "    'eval_metric':'mae',\n",
    "    'silent': 1\n",
    "}\n",
    "\n",
    "\n",
    "#Block of code used for hypertuning parameters. Adapt to each round of parameter tuning.\n",
    "#Turn off CV in submission\n",
    "CV=False\n",
    "if CV:\n",
    "    dtrain = xgb.DMatrix(X_train,label=Y)\n",
    "    gridsearch_params = {\n",
    "        'eta': [(eta) for eta in np.arange(.04, 0.3, .02)],\n",
    "        'max_depth': [(max_depth) for max_depth in np.arange(1,6,1)],\n",
    "        'min_child_weight': [(min_child_weight) for min_child_weight in np.arange(1,6,1)],\n",
    "        'subsample': [(subsample) for subsample in np.arange(0,1.1,0.5)],\n",
    "        'lambda': [(lambd) for lambd in np.geomspace(0.01,10,num=5)],\n",
    "        'alpha': [(alpha) for alpha in np.geomspace(0.01,10,num=5)]\n",
    "    }\n",
    "\n",
    "    # Define initial best params and MAE\n",
    "    min_mae = float(\"Inf\")\n",
    "    best_params = {}\n",
    "    \n",
    "    #searching for best eta\n",
    "    for param in gridsearch_params:\n",
    "        print(\"*******************************CV with param {} *******************************************\".format(\n",
    "                                 param))\n",
    "        for i in gridsearch_params[param]:\n",
    "            print('now at ', i)\n",
    "            # Update our parameters\n",
    "            params[param] = i\n",
    "\n",
    "            # Run CV\n",
    "            cv_results = xgb.cv(\n",
    "                params,\n",
    "                dtrain,\n",
    "                num_boost_round=100,\n",
    "                nfold=3,\n",
    "                metrics={'mae'},\n",
    "                early_stopping_rounds=10\n",
    "            )\n",
    "#             for result in cv_results:\n",
    "#                 print(cv_results[result])\n",
    "            # Update best MAE\n",
    "            mean_mae = cv_results['test-mae-mean'].min()\n",
    "            boost_rounds = cv_results['test-mae-mean'].argmin()\n",
    "            print(\"MAE {} for {} rounds\".format(mean_mae, boost_rounds+1))\n",
    "            if mean_mae < min_mae:\n",
    "                min_mae = mean_mae\n",
    "                print(set(gridsearch_params).intersection(set(params)))\n",
    "                for k in set(gridsearch_params).intersection(set(params)):\n",
    "                    best_params[k] = params[k]\n",
    "            params.pop(param, None)\n",
    "        if param in best_params:\n",
    "            params[param] = best_params[param]\n",
    "    \n",
    "    print(\"Best params: {}, MAE: {}\".format(best_params, min_mae))\n",
    "else:\n",
    "    #Print final params to use for the model\n",
    "    params['silent'] = 0 #Turn on output\n",
    "    print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7c35ede726c00a6abfd82eb3fe080b08124ed998"
   },
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6ed9f6044fb0fc8225d8ef28c00e354288da2914"
   },
   "source": [
    "### Training on all Data using params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5f11245685f59ec128944c49a61fb7ba5411f10b"
   },
   "outputs": [],
   "source": [
    "#Import data\n",
    "train = pd.read_csv('../input/train_V2.csv')\n",
    "X_test = pd.read_csv('../input/test_V2.csv')\n",
    "# Remove Id, which is not a useful feature\n",
    "train.drop(columns=['Id'], inplace=True)\n",
    "train = train.dropna()\n",
    "# One hot encode matchType\n",
    "train = pd.get_dummies(train, columns=['matchType'])\n",
    "X_test = pd.get_dummies(X_test, columns=['matchType'])\n",
    "# Turn groupId and match Id into categorical types\n",
    "train['groupId'] = train['groupId'].astype('category')\n",
    "train['matchId'] = train['matchId'].astype('category')\n",
    "X_test['groupId'] = X_test['groupId'].astype('category')\n",
    "X_test['matchId'] = X_test['matchId'].astype('category')\n",
    "\n",
    "# Get category coding for groupId and matchID\n",
    "train['groupId_cat'] = train['groupId'].cat.codes\n",
    "train['matchId_cat'] = train['matchId'].cat.codes\n",
    "X_test['groupId_cat'] = X_test['groupId'].cat.codes\n",
    "X_test['matchId_cat'] = X_test['matchId'].cat.codes\n",
    "\n",
    "# Get rid of old columns\n",
    "train.drop(columns=['groupId', 'matchId'], inplace=True)\n",
    "X_test.drop(columns=['groupId', 'matchId'], inplace=True)\n",
    "\n",
    "# Split train into features and target variable\n",
    "X_train = train.drop(columns = ['winPlacePerc']) #all columns except target\n",
    "Y = train['winPlacePerc'] # Only target variable\n",
    "Y = Y.astype('float32')\n",
    "\n",
    "#Standard scaling train features to have 0 mean and 1 variance\n",
    "columns_to_scale = ['assists', 'boosts', 'damageDealt', 'DBNOs', 'headshotKills', 'heals',\n",
    "       'killPlace', 'killPoints', 'kills', 'killStreaks', 'longestKill',\n",
    "       'matchDuration', 'maxPlace', 'numGroups', 'rankPoints', 'revives',\n",
    "       'rideDistance', 'roadKills', 'swimDistance', 'teamKills',\n",
    "       'vehicleDestroys', 'walkDistance', 'weaponsAcquired', 'winPoints']\n",
    "categorical  = ['matchType_crashfpp', 'matchType_crashtpp', 'matchType_duo',\n",
    "       'matchType_duo-fpp', 'matchType_flarefpp', 'matchType_flaretpp',\n",
    "       'matchType_normal-duo', 'matchType_normal-duo-fpp',\n",
    "       'matchType_normal-solo', 'matchType_normal-solo-fpp',\n",
    "       'matchType_normal-squad', 'matchType_normal-squad-fpp',\n",
    "       'matchType_solo', 'matchType_solo-fpp', 'matchType_squad',\n",
    "       'matchType_squad-fpp', 'groupId_cat', 'matchId_cat']\n",
    "train_scale = X_train[columns_to_scale]\n",
    "train_categorical = X_train[categorical]\n",
    "test_scale = X_test[columns_to_scale]\n",
    "test_categorical = X_test[categorical]\n",
    "# print(df_scale.shape)\n",
    "# print(df_categorical.shape)\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "scaler.fit(train_scale.values)\n",
    "\n",
    "#Standard scaling train features to have 0 mean and 1 variance\n",
    "train_scale = pd.DataFrame(scaler.transform(train_scale.values), index=train_scale.index, columns=train_scale.columns)\n",
    "# print(df_scale.shape)\n",
    "X_train = pd.concat([train_scale, train_categorical],axis=1)\n",
    "\n",
    "#Standard scaling test features with the same parameters in train data\n",
    "test_scale = pd.DataFrame(scaler.transform(test_scale.values), index=test_scale.index, columns=test_scale.columns)\n",
    "# print(df_scale.shape)\n",
    "\n",
    "test_id = X_test.loc[:, ['Id']]\n",
    "X_test = pd.concat([test_scale, test_categorical],axis=1)\n",
    "\n",
    "X_train = reduce_mem_usage(X_train)\n",
    "X_test = reduce_mem_usage(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a55fe24bf863af86951b638c0e04934e6853cd54"
   },
   "outputs": [],
   "source": [
    "matrix_train = xgb.DMatrix(X_train,label=Y)\n",
    "model=xgb.train(params=params,\n",
    "                dtrain=matrix_train,num_boost_round=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7cb1d5e55496ee8a0b3979399811bac9158a1641"
   },
   "outputs": [],
   "source": [
    "prediction = model.predict(xgb.DMatrix(X_test), ntree_limit = model.best_ntree_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ad821182f87f543f625d67f05817d465b22aeeb8"
   },
   "outputs": [],
   "source": [
    "prediction = prediction.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "056b7e8f88aec62a917d0bed30b7868015e9fd75"
   },
   "outputs": [],
   "source": [
    "df_sub = pd.read_csv(\"../input/sample_submission_V2.csv\")\n",
    "df_test = pd.read_csv(\"../input/test_V2.csv\")\n",
    "df_sub['winPlacePerc'] = prediction\n",
    "# Restore some columns\n",
    "df_sub = df_sub.merge(df_test[[\"Id\", \"matchId\", \"groupId\", \"maxPlace\", \"numGroups\"]], on=\"Id\", how=\"left\")\n",
    "\n",
    "# Sort, rank, and assign adjusted ratio\n",
    "df_sub_group = df_sub.groupby([\"matchId\", \"groupId\"]).first().reset_index()\n",
    "df_sub_group[\"rank\"] = df_sub_group.groupby([\"matchId\"])[\"winPlacePerc\"].rank()\n",
    "df_sub_group = df_sub_group.merge(\n",
    "    df_sub_group.groupby(\"matchId\")[\"rank\"].max().to_frame(\"max_rank\").reset_index(), \n",
    "    on=\"matchId\", how=\"left\")\n",
    "df_sub_group[\"adjusted_perc\"] = (df_sub_group[\"rank\"] - 1) / (df_sub_group[\"numGroups\"] - 1)\n",
    "\n",
    "df_sub = df_sub.merge(df_sub_group[[\"adjusted_perc\", \"matchId\", \"groupId\"]], on=[\"matchId\", \"groupId\"], how=\"left\")\n",
    "df_sub[\"winPlacePerc\"] = df_sub[\"adjusted_perc\"]\n",
    "\n",
    "# Deal with edge cases\n",
    "df_sub.loc[df_sub.maxPlace == 0, \"winPlacePerc\"] = 0\n",
    "df_sub.loc[df_sub.maxPlace == 1, \"winPlacePerc\"] = 1\n",
    "\n",
    "# Align with maxPlace\n",
    "# Credit: https://www.kaggle.com/anycode/simple-nn-baseline-4\n",
    "subset = df_sub.loc[df_sub.maxPlace > 1]\n",
    "gap = 1.0 / (subset.maxPlace.values - 1)\n",
    "new_perc = np.around(subset.winPlacePerc.values / gap) * gap\n",
    "df_sub.loc[df_sub.maxPlace > 1, \"winPlacePerc\"] = new_perc\n",
    "\n",
    "# Edge case\n",
    "df_sub.loc[(df_sub.maxPlace > 1) & (df_sub.numGroups == 1), \"winPlacePerc\"] = 0\n",
    "assert df_sub[\"winPlacePerc\"].isnull().sum() == 0\n",
    "\n",
    "df_sub[[\"Id\", \"winPlacePerc\"]].to_csv(\"submission_adjusted.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f0165352aa8d86671bf6fcfb688e200684994241"
   },
   "outputs": [],
   "source": [
    "# imp = pd.DataFrame(list(model.get_fscore().items()), columns=['cols', 'imp'])\n",
    "# imp['imp'] = imp['imp'] / imp['imp'].sum()\n",
    "# imp = imp.sort_values('imp', ascending=False)\n",
    "# for i in range(imp.shape[0]):\n",
    "#     print(imp['imp'][:i+1].sum())\n",
    "#     if imp['imp'][:i+1].sum() > 0.9:\n",
    "#         max_features = i\n",
    "#         break\n",
    "# max_features\n",
    "# imp = imp[:max_features]\n",
    "# imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "15b53e4f39d560c6d48e65e019407669caf5b5e6"
   },
   "outputs": [],
   "source": [
    "# x_traink = x_train[imp['cols']]\n",
    "# x_valk = x_val[imp['cols']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "543b2d50ecef746a85c51e46b7b6c4d9be02fcb3"
   },
   "outputs": [],
   "source": [
    "# model = XGBmodel(x_traink,x_valk,y_train,y_val,params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2ef1336c74add0c2a702b1199b22e1a44fcf5c42"
   },
   "outputs": [],
   "source": [
    "# trees = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
    "#              learning_rate=1, loss='ls', max_depth=3, max_features=None,\n",
    "#              max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "#              min_impurity_split=None, min_samples_leaf=1,\n",
    "#              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "#              n_estimators=1, presort='auto', random_state=None,\n",
    "#              subsample=1.0, verbose=0, warm_start=False)\n",
    "# trees.fit(x_traink, y_train)\n",
    "# y_pred = trees.predict(x_valk)\n",
    "# mae = sklearn.metrics.mean_absolute_error(y_pred, y_val)\n",
    "# mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5276f4f03028af60ed3453b9d8b0b442e8fd262f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6bde95b41165396a603d822723c475aaef6ba576"
   },
   "outputs": [],
   "source": [
    "# m2 = RandomForestRegressor(n_estimators=80, min_samples_leaf=3, max_features='sqrt',\n",
    "#                           n_jobs=-1)\n",
    "# m2.fit(x_traink, y_train)\n",
    "# y_pred = m2.predict(x_valk)\n",
    "# mae = sklearn.metrics.mean_absolute_error(y_pred, y_val)\n",
    "# mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c1bba2c0be70c97964cd6ed348d8154e6aa45647"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a1252a35933fcd84f675fe550f5563b751ebcd45"
   },
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "09b0df6da355f4ba78c1feec3ab01c3b2248c9af"
   },
   "outputs": [],
   "source": [
    "# params = {\"objective\" : \"regression\", \"metric\" : \"mae\", 'n_estimators':20000, 'early_stopping_rounds':200,\n",
    "#               \"num_leaves\" : 31, \"learning_rate\" : 0.05, \"bagging_fraction\" : 0.7,\n",
    "#                \"bagging_seed\" : 0, \"num_threads\" : 4,\"colsample_bytree\" : 0.7\n",
    "#              }\n",
    "    \n",
    "# lgtrain = lgb.Dataset(x_train, label=y_train)\n",
    "# lgval = lgb.Dataset(x_val, label=y_val)\n",
    "# model = lgb.train(params, lgtrain, valid_sets=[lgtrain, lgval], early_stopping_rounds=200, verbose_eval=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a09fdfb186bf811c90c510799b6942bb10305b9b"
   },
   "outputs": [],
   "source": [
    "# params = {\"objective\" : \"regression\", \"metric\" : \"mae\", 'n_estimators':20000, 'early_stopping_rounds':200,\n",
    "#               \"num_leaves\" : 31, \"learning_rate\" : 0.05, \"bagging_fraction\" : 0.7,\n",
    "#                \"bagging_seed\" : 0, \"num_threads\" : 4,\"colsample_bytree\" : 0.7\n",
    "#              }\n",
    "    \n",
    "# lgtrain = lgb.Dataset(x_traink, label=y_train)\n",
    "# lgval = lgb.Dataset(x_valk, label=y_val)\n",
    "# model = lgb.train(params, lgtrain, valid_sets=[lgtrain, lgval], early_stopping_rounds=200, verbose_eval=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b9efbc334b75d52cd05db33f0cdb80ac81284591"
   },
   "outputs": [],
   "source": [
    "# pred_test_y = model.predict(X_test, num_iteration=model.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b498c0a1d33894bc7823939bb25997ffbbdb6929"
   },
   "outputs": [],
   "source": [
    "# df_sub = pd.read_csv(\"../input/sample_submission_V2.csv\")\n",
    "# df_test = pd.read_csv(\"../input/test_V2.csv\")\n",
    "# df_sub['winPlacePerc'] = pred_test_y\n",
    "# # Restore some columns\n",
    "# df_sub = df_sub.merge(df_test[[\"Id\", \"matchId\", \"groupId\", \"maxPlace\", \"numGroups\"]], on=\"Id\", how=\"left\")\n",
    "\n",
    "# # Sort, rank, and assign adjusted ratio\n",
    "# df_sub_group = df_sub.groupby([\"matchId\", \"groupId\"]).first().reset_index()\n",
    "# df_sub_group[\"rank\"] = df_sub_group.groupby([\"matchId\"])[\"winPlacePerc\"].rank()\n",
    "# df_sub_group = df_sub_group.merge(\n",
    "#     df_sub_group.groupby(\"matchId\")[\"rank\"].max().to_frame(\"max_rank\").reset_index(), \n",
    "#     on=\"matchId\", how=\"left\")\n",
    "# df_sub_group[\"adjusted_perc\"] = (df_sub_group[\"rank\"] - 1) / (df_sub_group[\"numGroups\"] - 1)\n",
    "\n",
    "# df_sub = df_sub.merge(df_sub_group[[\"adjusted_perc\", \"matchId\", \"groupId\"]], on=[\"matchId\", \"groupId\"], how=\"left\")\n",
    "# df_sub[\"winPlacePerc\"] = df_sub[\"adjusted_perc\"]\n",
    "\n",
    "# # Deal with edge cases\n",
    "# df_sub.loc[df_sub.maxPlace == 0, \"winPlacePerc\"] = 0\n",
    "# df_sub.loc[df_sub.maxPlace == 1, \"winPlacePerc\"] = 1\n",
    "\n",
    "# # Align with maxPlace\n",
    "# # Credit: https://www.kaggle.com/anycode/simple-nn-baseline-4\n",
    "# subset = df_sub.loc[df_sub.maxPlace > 1]\n",
    "# gap = 1.0 / (subset.maxPlace.values - 1)\n",
    "# new_perc = np.around(subset.winPlacePerc.values / gap) * gap\n",
    "# df_sub.loc[df_sub.maxPlace > 1, \"winPlacePerc\"] = new_perc\n",
    "\n",
    "# # Edge case\n",
    "# df_sub.loc[(df_sub.maxPlace > 1) & (df_sub.numGroups == 1), \"winPlacePerc\"] = 0\n",
    "# assert df_sub[\"winPlacePerc\"].isnull().sum() == 0\n",
    "\n",
    "# df_sub[[\"Id\", \"winPlacePerc\"]].to_csv(\"submission_adjusted.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "22b8e1988d956578acf5300e10cf7ad1263fbfb2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
